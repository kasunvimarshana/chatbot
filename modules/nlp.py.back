import re
import spacy
from typing import List, Optional, Callable, Union
import logging
from functools import partial

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TextPreprocessor:
    """A comprehensive text preprocessing pipeline"""
    
    def __init__(
        self,
        spacy_model: str = "en_core_web_sm",
        disable_spacy_components: List[str] = ["parser", "ner"],
        min_token_length: int = 2,
        keep_numbers: bool = False,
        custom_stopwords: Optional[List[str]] = None,
        custom_filters: Optional[List[Callable]] = None
    ):
        """
        Initialize the text preprocessor
        
        Args:
            spacy_model: Name of spaCy model to use
            disable_spacy_components: spaCy components to disable for efficiency
            min_token_length: Minimum length of tokens to keep
            keep_numbers: Whether to keep numeric tokens
            custom_stopwords: Additional stopwords to add
            custom_filters: Additional filter functions to apply
        """
        self.spacy_model = spacy_model
        self.disable_spacy_components = disable_spacy_components
        self.min_token_length = min_token_length
        self.keep_numbers = keep_numbers
        
        # Initialize spaCy model placeholder
        self.nlp = None
        
        # Load spaCy model with error handling
        self._load_spacy_model()
        
        # Initialize default stopwords
        self.stopwords = self.nlp.Defaults.stop_words if self.nlp else set()
        
        # Add custom stopwords if provided
        if custom_stopwords:
            self.stopwords.update(set(custom_stopwords))
            logger.info(f"Added {len(custom_stopwords)} custom stopwords")
        
        # Initialize processing pipeline
        self.filters = [
            self._normalize_whitespace_filter,
            self._remove_special_chars_filter,
            self._expand_contractions_filter,
            self._lemmatize_filter,
            self._filter_tokens_filter
        ]
        
        # Add custom filters if provided
        if custom_filters:
            self.filters.extend(custom_filters)
            logger.info(f"Added {len(custom_filters)} custom filters")
    
    def _load_spacy_model(self):
        """Load spaCy model with proper error handling"""
        try:
            import spacy
            self.nlp = spacy.load(
                self.spacy_model, 
                disable=self.disable_spacy_components
            )
            logger.info(f"Successfully loaded spaCy model: {self.spacy_model}")
        except ImportError:
            logger.error("spaCy package not installed. Please install with: pip install spacy")
            raise
        except OSError:
            logger.error(f"spaCy model {self.spacy_model} not found. Attempting to download...")
            try:
                import spacy.cli
                spacy.cli.download(self.spacy_model)
                self.nlp = spacy.load(
                    self.spacy_model, 
                    disable=self.disable_spacy_components
                )
                logger.info(f"Successfully downloaded and loaded spaCy model: {self.spacy_model}")
            except Exception as e:
                logger.error(f"Failed to download spaCy model: {str(e)}")
                raise RuntimeError(f"Could not load or download spaCy model: {self.spacy_model}") from e
    
    def preprocess_text(self, text: str) -> str:
        """Basic text cleaning"""
        if not isinstance(text, str):
            logger.warning(f"Non-string input received: {type(text)}")
            return ""
        
        text = text.lower().strip()
        text = self._normalize_whitespace(text)
        text = self._remove_special_chars(text)
        text = self._expand_contractions(text)
        return text
    
    def process(self, text: str, as_list: bool = False) -> Union[str, List[str]]:
        """
        Process input text through the complete pipeline
        
        Args:
            text: Input text to process
            as_list: Return tokens as list instead of joined string
            
        Returns:
            Processed text or token list
        """
        try:
            if not text or not isinstance(text, str):
                logger.warning("Empty or invalid input text")
                return [] if as_list else ""
            
            if not self.nlp:
                logger.error("spaCy model not loaded - cannot process text")
                return text
            
            # Basic cleaning
            cleaned_text = self.preprocess_text(text)
            
            # Advanced processing with spaCy
            doc = self.nlp(cleaned_text)
            
            # Apply all filters in sequence
            tokens = []
            for token in doc:
                processed_token = token.text
                for filter_func in self.filters:
                    processed_token = filter_func(token, processed_token)
                    if not processed_token:
                        break
                
                if processed_token:
                    tokens.append(processed_token)
            
            return tokens if as_list else ' '.join(tokens)
        
        except Exception as e:
            logger.error(f"Error processing text: {str(e)}", exc_info=True)
            return [] if as_list else text
    
    # Standalone text processing methods
    def _normalize_whitespace(self, text: str) -> str:
        """Normalize whitespace in text"""
        return re.sub(r'\s+', ' ', text).strip()
    
    def _remove_special_chars(self, text: str) -> str:
        """Remove special characters while preserving basic punctuation"""
        return re.sub(r'[^a-zA-Z0-9\s.,!?]', '', text)
    
    def _expand_contractions(self, text: str) -> str:
        """Expand common English contractions"""
        contractions = {
            r"won't": "will not",
            r"can't": "cannot",
            r"i'm": "i am",
            r"ain't": "is not",
            r"\'ll": " will",
            r"\'ve": " have",
            r"\'re": " are",
            r"\'d": " would",
            r"n\'t": " not"
        }
        for pattern, replacement in contractions.items():
            text = re.sub(pattern, replacement, text)
        return text
    
    # Filter methods for token processing
    def _normalize_whitespace_filter(self, token, current_text: str) -> str:
        """Filter version of whitespace normalizer"""
        return self._normalize_whitespace(current_text)
    
    def _remove_special_chars_filter(self, token, current_text: str) -> str:
        """Filter version of special chars remover"""
        return self._remove_special_chars(current_text)
    
    def _expand_contractions_filter(self, token, current_text: str) -> str:
        """Filter version of contractions expander"""
        return self._expand_contractions(current_text)
    
    def _lemmatize_filter(self, token, current_text: str) -> str:
        """Lemmatize token if it's not a stopword"""
        if not self.nlp:
            return current_text
            
        if token.text in self.stopwords:
            return ""
        return token.lemma_.lower()
    
    def _filter_tokens_filter(self, token, current_text: str) -> str:
        """Apply final token filtering"""
        if not current_text:
            return ""
        
        # Length filter
        if len(current_text) < self.min_token_length:
            return ""
        
        # Alpha/numeric filter
        if not self.keep_numbers and token.like_num:
            return ""
        
        return current_text


if __name__ == "__main__":
    try:
        # Initialize with custom settings
        preprocessor = TextPreprocessor(
            custom_stopwords=["example", "sample"],
            keep_numbers=False,
            min_token_length=3
        )
        
        test_texts = [
            "I'm testing this preprocessor with some contractions!",
            "This is an example sentence with numbers like 123 and special#chars.",
            "",
            12345,  # Non-string input
            "Short words are filtered out by default"
        ]
        
        for text in test_texts:
            processed = preprocessor.process(text)
            logger.info(f"Original: {text}")
            logger.info(f"Processed: {processed}")
    
    except Exception as e:
        logger.error(f"Error in example usage: {str(e)}", exc_info=True)
        raise